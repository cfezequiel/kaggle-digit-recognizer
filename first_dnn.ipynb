{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TRAIN_STEPS = 1000\n",
    "TRAIN_SAMPLES = 33600\n",
    "TRAIN_CSV = 'data/train.csv'\n",
    "TEST_CSV = 'data/test.csv'\n",
    "OUTPUT_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "def import_data(csv_filepath, nosplit=False, **kwargs):\n",
    "    df = pd.read_csv(csv_filepath, **kwargs)\n",
    "    if nosplit:\n",
    "        return df\n",
    "    \n",
    "    dfx = df.ix[:, 1:]\n",
    "    dfy = df.ix[:, 0]\n",
    "    \n",
    "    return dfx, dfy\n",
    "\n",
    "# Train data: 42K samples\n",
    "# Split this into train and validation sets\n",
    "dfx, dfy = import_data(TRAIN_CSV, nrows=TRAIN_SAMPLES)\n",
    "dfx_cv, dfy_cv = import_data(TRAIN_CSV, skiprows=TRAIN_SAMPLES)\n",
    "\n",
    "# Test data: 28K samples\n",
    "df_test = import_data(TEST_CSV, nosplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "def preproc_x(dfx):\n",
    "    x = min_max_scaler.fit_transform(dfx.values).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def preproc_y(dfy):\n",
    "    y = dfy.values.astype(np.int32)\n",
    "    return y\n",
    "\n",
    "x = preproc_x(dfx)\n",
    "y = preproc_y(dfy)\n",
    "x_cv = preproc_x(dfx_cv)\n",
    "y_cv = preproc_y(dfy_cv)\n",
    "x_test = preproc_x(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x122644890>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Build the classifier\n",
    "import tensorflow as tf\n",
    "\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(x)\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[10, 20, 10],\n",
    "    n_classes=len(dfy_.unique()),\n",
    "    model_dir='/tmp/mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/carlos/virtualenvs/default/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 2002 into /tmp/mnist_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.09114, step = 2002\n",
      "INFO:tensorflow:global_step/sec: 6.47893\n",
      "INFO:tensorflow:loss = 0.474986, step = 2102\n",
      "INFO:tensorflow:global_step/sec: 6.2635\n",
      "INFO:tensorflow:loss = 0.392118, step = 2202\n",
      "INFO:tensorflow:global_step/sec: 6.64297\n",
      "INFO:tensorflow:loss = 0.349716, step = 2302\n",
      "INFO:tensorflow:global_step/sec: 6.60319\n",
      "INFO:tensorflow:loss = 0.324104, step = 2402\n",
      "INFO:tensorflow:global_step/sec: 6.73643\n",
      "INFO:tensorflow:loss = 0.305742, step = 2502\n",
      "INFO:tensorflow:global_step/sec: 6.58223\n",
      "INFO:tensorflow:loss = 0.291197, step = 2602\n",
      "INFO:tensorflow:global_step/sec: 6.53323\n",
      "INFO:tensorflow:loss = 0.279175, step = 2702\n",
      "INFO:tensorflow:global_step/sec: 6.02484\n",
      "INFO:tensorflow:loss = 0.268911, step = 2802\n",
      "INFO:tensorflow:global_step/sec: 5.56858\n",
      "INFO:tensorflow:loss = 0.259763, step = 2902\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /tmp/mnist_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.251713.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x11193af50>, 'hidden_units': [10, 20, 10], 'feature_columns': (_RealValuedColumn(column_name='', dimension=784, default_value=None, dtype=tf.float32, normalizer=None),), 'embedding_lr_multipliers': None, 'optimizer': None, 'dropout': None, 'gradient_clip_norm': None, 'activation_fn': <function relu at 0x11b328d70>, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier\n",
    "classifier.fit(\n",
    "    input_fn=lambda: (\n",
    "        tf.constant(x),\n",
    "        tf.constant(y)\n",
    "    ),\n",
    "    steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/carlos/virtualenvs/default/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-13-15:18:51\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-13-15:18:53\n",
      "INFO:tensorflow:Saving dict for global step 3001: accuracy = 0.916429, auc = 0.991998, global_step = 3001, loss = 0.289948\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "Cross validation accuracy: 0.916429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy_score = classifier.evaluate(\n",
    "    input_fn=lambda: (\n",
    "        tf.constant(x_cv),\n",
    "        tf.constant(y_cv)\n",
    "    ),\n",
    "    steps=1)['accuracy']\n",
    "print(\"\\nCross validation accuracy: {0:f}\\n\".format(accuracy_score))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions using test data\n",
    "y_test = classifier.predict(input_fn=lambda: tf.constant(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the output file\n",
    "import csv\n",
    "import datetime\n",
    "from os import path\n",
    "\n",
    "timestamp_str = (str(datetime.datetime.now())\n",
    "                 .replace(' ', '_')\n",
    "                 .replace(':', '-'))\n",
    "filename = 'output_{}.csv'.format(timestamp_str)\n",
    "filepath = path.join(OUTPUT_DIR, filename)\n",
    "with open(filepath, 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['ImageId', 'Label'])\n",
    "    for i, result in enumerate(y_test, 1):\n",
    "        w.writerow([i, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check output file\n",
    "out = pd.read_csv(filepath)\n",
    "assert len(out) == 28000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
